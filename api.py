import os
import sys
import uuid
import torch
import librosa
import numpy as np
import soundfile as sf
import tempfile
import shutil
from pathlib import Path
from flask import Flask, request, send_file, jsonify
import imageio_ffmpeg as im_ffmpeg
from moviepy import VideoFileClip
from audio_separator.separator import Separator
from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor

# --- 1. ç¯å¢ƒä¸ç¡¬ä»¶é…ç½® ---
ffmpeg_exe = im_ffmpeg.get_ffmpeg_exe()
os.environ["PATH"] = os.path.dirname(ffmpeg_exe) + os.pathsep + os.environ["PATH"]
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_CACHE = Path(os.path.expanduser("~")) / ".male_voice_remover" / "models"
MODEL_CACHE.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR = Path("D:/male_voice_remover/outputs")
OUTPUT_DIR.mkdir(exist_ok=True)

# --- 2. å…¨å±€æ¨¡å‹åˆå§‹åŒ– (åœ¨ API å¯åŠ¨å‰åŠ è½½åˆ°å†…å­˜) ---
print(f"[INIT] æ­£åœ¨é¢„åŠ è½½ AI æ€§åˆ«æ£€æµ‹æ¨¡å‹è‡³ {DEVICE}...")
GENDER_MODEL_NAME = "prithivMLmods/Common-Voice-Gender-Detection"
gender_extractor = Wav2Vec2FeatureExtractor.from_pretrained(GENDER_MODEL_NAME)
gender_model = Wav2Vec2ForSequenceClassification.from_pretrained(GENDER_MODEL_NAME).to(
    DEVICE
)
gender_model.eval()

app = Flask(__name__)


@app.route("/process", methods=["POST"])
def process():
    """
    æ¥æ”¶è§†é¢‘æ–‡ä»¶ï¼Œæ‰§è¡Œå®Œæ•´çš„ AI åˆ†ç¦»å’Œè¿‡æ»¤ç®¡çº¿ï¼Œè¿”å› WAVã€‚
    """
    if "video" not in request.files:
        return jsonify({"error": "Missing 'video' file in request"}), 400

    task_id = str(uuid.uuid4())[:8]
    temp_dir = Path(tempfile.mkdtemp())
    video_file = request.files["video"]
    input_video_path = temp_dir / f"input_{task_id}.mp4"

    try:
        # 1. ä¿å­˜ä¸Šä¼ æ–‡ä»¶
        print(f"[{task_id}] æ¥æ”¶åˆ°è¯·æ±‚ï¼Œä¿å­˜è§†é¢‘...")
        video_file.save(str(input_video_path))

        # 2. æå–åŸå§‹éŸ³é¢‘
        print(f"[{task_id}] æå–è§†é¢‘éŸ³é¢‘è½¨é“...")
        raw_audio = temp_dir / "raw.wav"
        with VideoFileClip(str(input_video_path)) as video:
            if video.audio is None:
                return jsonify({"error": "Video has no audio track"}), 400
            video.audio.write_audiofile(str(raw_audio), fps=44100, logger=None)

        # 3. AI äººå£°ä¼´å¥åˆ†ç¦» (MDX-NET)
        print(f"[{task_id}] æ­£åœ¨é€šè¿‡ AI åˆ†ç¦»äººå£°å’Œä¼´å¥...")
        separator = Separator(output_dir=str(temp_dir), model_file_dir=str(MODEL_CACHE))
        separator.load_model("UVR-MDX-NET-Voc_FT.onnx")
        separated_files = separator.separate(str(raw_audio))

        vocal_path = None
        inst_path = None
        for f in separated_files:
            full_f = temp_dir / f
            if "vocals" in f.lower():
                vocal_path = full_f
            else:
                inst_path = full_f

        # 4. æ€§åˆ«æ£€æµ‹ä¸é™éŸ³è¿‡æ»¤
        print(f"[{task_id}] æ­£åœ¨è¯†åˆ«å¹¶å‰”é™¤ç”·æ€§å£°éŸ³...")
        v_audio, sr = librosa.load(str(vocal_path), sr=None)
        v_16k, _ = librosa.load(str(vocal_path), sr=16000)  # æ€§åˆ«æ¨¡å‹å¿…é¡»ç”¨ 16k

        seg_len = 16000 * 1  # æ¯ 1 ç§’æ£€æµ‹ä¸€æ¬¡
        mask = np.ones(len(v_16k), dtype=np.float32)

        for i in range(0, len(v_16k), seg_len):
            segment = v_16k[i : i + seg_len]
            if len(segment) < 16000:
                continue

            inputs = gender_extractor(
                segment, sampling_rate=16000, return_tensors="pt", padding=True
            ).to(DEVICE)
            with torch.no_grad():
                logits = gender_model(**inputs).logits
                is_male = torch.argmax(logits).item() == 1

            if is_male:
                mask[i : i + seg_len] = 0.0  # ç”·æ€§å£°éŸ³è®¾ä¸ºå®Œå…¨é™éŸ³

        # å¯¹é½æ©ç åˆ°åŸå§‹é‡‡æ ·ç‡
        mask_full = librosa.resample(mask, orig_sr=16000, target_sr=sr)
        min_len = min(len(v_audio), len(mask_full))
        filtered_vocals = v_audio[:min_len] * mask_full[:min_len]

        # 5. æœ€ç»ˆæ··ç¼© (è¿‡æ»¤åçš„äººå£° + åŸå§‹ä¼´å¥)
        print(f"[{task_id}] æ­£åœ¨åˆæˆæœ€ç»ˆéŸ³é¢‘...")
        inst_audio, _ = librosa.load(str(inst_path), sr=sr)
        min_mix = min(len(filtered_vocals), len(inst_audio))
        final_mixed = filtered_vocals[:min_mix] + inst_audio[:min_mix]

        output_filename = f"api_result_{task_id}.wav"
        final_output_path = OUTPUT_DIR / output_filename
        sf.write(str(final_output_path), final_mixed, sr)

        print(f"[{task_id}] å¤„ç†å®Œæˆï¼Œæ–‡ä»¶å·²å­˜è‡³: {final_output_path}")
        return send_file(
            str(final_output_path),
            as_attachment=True,
            download_name=f"no_male_voice_{task_id}.wav",
        )

    except Exception as e:
        print(f"[{task_id}] è¿è¡Œå´©æºƒ: {str(e)}")
        return jsonify({"error": str(e)}), 500
    finally:
        # æ¸…ç†ä¸´æ—¶ä»»åŠ¡ç›®å½•
        shutil.rmtree(temp_dir, ignore_errors=True)


if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("ğŸš€ é›†æˆåŒ– AI è§†é¢‘è½¬éŸ³é¢‘ API å·²å¯åŠ¨")
    print("ğŸ“ æ¥å£åœ°å€: http://127.0.0.1:5001/process")
    print(
        "ğŸ§  ç¡¬ä»¶åŠ é€Ÿ: "
        + ("Enabled (GPU)" if torch.cuda.is_available() else "Disabled (CPU)")
    )
    print("=" * 60 + "\n")
    app.run(host="0.0.0.0", port=5001, threaded=True)
